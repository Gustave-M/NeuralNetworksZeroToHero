{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dccb4786",
   "metadata": {},
   "source": [
    "Tiktokenizer\n",
    "\n",
    "toke, tokenisation, \n",
    "Juste faire pour minimiser le nomber de token d'un texte classique ? (logique mais pas nécéssairement vrai)\n",
    "embeding_table utilise les tokens\n",
    "\n",
    "Exemple des token pour le code en python, plusieurs espace en 1 seul token c'est mieux, un seul llm c'est pas assez (fin embeding table quoi)\n",
    "\n",
    "unicode code point, unicode mais en ++, en python ord(\"c\") pour sin int ubicode code point\n",
    "\n",
    "22min\n",
    "\n",
    "\n",
    "Byte Pair Encoding (BPE), comme en MP2I lol, c'est un petit algo de compression\n",
    "\n",
    "llm et tokeniser sont indépendant, c'est une étape initiale et certainement des training-set différents\n",
    "\n",
    "46min\n",
    "\n",
    "Ok, le fait de coupler par plus haute fréquence n'est pas optimal, cf son exemple avec la ponctuation, e., e!, e?, nous n'avons pas besoin de faire ces raprochement même si fréquent (car après plein de token avec '?' ...) ainsi nous ne voulons pas faire des tokens avec '?' ou '!'\n",
    "\n",
    "tokeniser = encode + decode\n",
    "\n",
    "Librairie regex (trop cool ça, souvenir de prépa)\n",
    "\n",
    "download file : wget https://.../gpt-2/models/1558M/vocab.bpe\n",
    "\n",
    "Tokens spéciaux, <endOfText>\n",
    "\n",
    "tiktoken lbrary en rust !\n",
    "\n",
    "FIM = Fill-in-the-Middle\n",
    "Lire l'article https://arxiv.org/abs/2207.14255\n",
    "\n",
    "Après l'ajoute de tokens spéciaux, il faut revenir sur le transfeormer, un vecteur de l'embeding pour ces tokens, quelque que 'model surgery'\n",
    "\n",
    "Sentence piece : à revoir, 1h30\n",
    "\n",
    "normalisation : retire les trucs crad genre '  ' mais pas toujours bien vu\n",
    "\n",
    "Utilisation de bytes et ede <unk> pour traiter les caractère inconnu quand même ... c'est byte_fallback\n",
    "\n",
    "add_duby_prefixe pour le problème de \" mot\" et \"mot\", ... pas efficace ..., en gros ajouter un ' ' au début\n",
    "\n",
    "1h45, parle des token, combien, trop, pas assez (du bon sens)\n",
    "\n",
    "Idée : basé la construction des tokens sur l'idée de minimiser la variace de fréquences d'utilisations des tokens dans la training set\n",
    "\n",
    "Lire : https://arxiv.org/abs/2304.08467\n",
    "\n",
    "Exemple de renverser une phase avec et sans passage par une liste (token de mot puis de letter)\n",
    "\n",
    "SolidGoldMajicarp ... un token jamais utilisé => résultat random (trop de token)\n",
    "\n",
    "JSON à bcp de token mais YAML (moins de token) (160 VS 100)\n",
    "\n",
    "\n",
    "\n",
    "Token pour un signal audio, comment ça marche ? Comme image ?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
