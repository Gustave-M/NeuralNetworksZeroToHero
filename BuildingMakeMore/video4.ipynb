{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c4f50dc",
   "metadata": {},
   "source": [
    "Ok, pour l'initialisation il faut être bien plus propre\n",
    "biais à 0 et Weight * petit (0.001 ? (évite 0 :))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b587dd7c",
   "metadata": {},
   "source": [
    "lisser l'action de tanh (autre fonction d'activation ? tanh plus longue ? (je me comprend))\n",
    "Ou bien modifier la préactivation (ce qui rentre dans tanh ?)\n",
    "\n",
    "Idée : tanh large puis qui s'affine avec le temps ...\n",
    "\n",
    "Ok, pour W, il faut diviser par sqrt(n) (je vois vite fait)\n",
    "Prendre en compte un coefficient gain en fonction de la fonction d'activation (sqrt(2) pour relu ..., 5/3? pour tanh)\n",
    "\n",
    "Maintenant, c'est moins important, il y à :\n",
    "- residual connexion\n",
    "- normalizations layers\n",
    "- better optimizer ...\n",
    "\n",
    "50min, pas tout tout compris\n",
    "55min, pas compris, pourquoi normaliser à la fin ?\n",
    "Oui, en fait j'ai pas compris batch normalisation ...\n",
    "\n",
    "BatchNormalization !!, normaliser les data d'enrtée ? (commme différentiable ...)\n",
    "\n",
    "Pour moi c'est la vidéo la plus complexe MAIS surtout la plus utile !! (à revoir !)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
